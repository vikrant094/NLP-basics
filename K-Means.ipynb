{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a KMeans clustering algorithm that clusters similar documents together. We shall use the 'nlp', 'philosophy' data from yesterday as demo and `train_finance.txt`, `train_medicine.txt`, `train_sports.txt`, `test_finance.txt`, `test_medicine.txt`, `test_sports.txt` for exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir('sample_data/') #change directory to where the folders are\n",
    "folders = glob.glob('*') #load all the folder names into a list\n",
    "# print(folders)\n",
    "\n",
    "all_texts = []\n",
    "all_categories = []\n",
    "\n",
    "for folder in folders:\n",
    "    print('importing text files from \"{}\" folder...'.format(folder), end=' ')\n",
    "    \n",
    "    files_in_folder = glob.glob(folder+'/*.txt')\n",
    "    \n",
    "    for _file_ in files_in_folder:\n",
    "        with open(_file_, 'r', encoding='latin-1') as f:\n",
    "            text_in_file = f.read()\n",
    "            all_texts.append(text_in_file)\n",
    "            all_categories.append(folder)\n",
    "            \n",
    "    print('found {} files'.format(len(files_in_folder)))\n",
    "        \n",
    "os.chdir('../') #revert back to original working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stopwords = nltk.corpus.stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def basic_preprocessing(text):\n",
    "    text = text.lower() #lowering\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) #removing all instances of citation brackets found in wiki articles\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in eng_stopwords] #removing stop words\n",
    "    text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return(text)\n",
    "\n",
    "processed_texts = [basic_preprocessing(text) for text in all_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_texts = all_texts[0:3] + all_texts[10:13]\n",
    "test_data = processed_texts[0:3] + processed_texts[10:13]\n",
    "test_target = all_categories[0:3] + all_categories[10:13]\n",
    "\n",
    "train_texts = all_texts[3:10] + all_texts[13:20]\n",
    "train_data = processed_texts[3:10] + processed_texts[13:20]\n",
    "train_target = all_categories[3:10] + all_categories[13:20]\n",
    "\n",
    "\n",
    "train_data = list(zip(train_data, train_target))\n",
    "\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### K-Means Model\n",
    "* Create a TF-IDF Matrix on the train data\n",
    "* Input this to a KMeans object\n",
    "\n",
    "```python\n",
    "km = KMeans(n_clusters=k)\n",
    "clusters = km.fit(train_tfidf)\n",
    "```\n",
    "\n",
    "This applies kmeans on tfidf data and stores the cluster centers\n",
    "\n",
    "* Predict clusters on train and test data\n",
    "```python\n",
    "train_clusters = clusters.predict(train_tfidf)\n",
    "test_clusters = clusters.predict(test_tfidf)\n",
    "```  \n",
    "\n",
    "This predicts which cluster each datapoint (row) belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(train_texts)\n",
    "train_tfidf = tfidf.transform(train_texts)\n",
    "\n",
    "km = KMeans(n_clusters=2) #Specify how many clusters must be built\n",
    "clusters = km.fit(train_tfidf) #This runs K-Means algo and decides where the cluster centroids are\n",
    "print('Train document clusters {}'.format(clusters.predict(train_tfidf)))\n",
    "#here 0 stands for one cluster and 1 stands for another\n",
    "\n",
    "## Test data\n",
    "test_tfidf = tfidf.transform(test_texts)\n",
    "print('Test document clusters: {}'.format(clusters.predict(test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "Train document clusters [1,1,1,1,1,1,1,0,0,0,0,0,0,0]\n",
    "```\n",
    "This means, the first document in train is at cluster 1 and the last document at cluster 0.  \n",
    "Understand that the numbers don't mean anything by themselves. What is important is the relative arrangement of the documents. We know that the first 7 documents in train were related to nlp category and that is reflected as first 7 documents being clustered in #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (Optional) Computing Distances between two documents\n",
    "\n",
    "As we know that the distance in K-Means is computed using Euclidean formula, we shall have a glance of how far the `data_point=10` is from the rest of the documents using two for loops. One for cluster 1 and other for cluster 2. Intuitively, we should expect smaller distances from cluster 2 since `data_point=10` is in cluster 2.\n",
    "\n",
    "We then plot both the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "all_distances_1 = []\n",
    "all_distances_2 = []\n",
    "data_point = 10\n",
    "\n",
    "for i in range(7):\n",
    "    all_distances_1.append(euclidean_distances(train_tfidf[data_point], train_tfidf[i])[0][0])\n",
    "\n",
    "for i in range(7,14):\n",
    "    all_distances_2.append(euclidean_distances(train_tfidf[data_point], train_tfidf[i])[0][0])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(7), all_distances_1, color='r', label='distance of data point {} from cluster 1 documents'.format(data_point))\n",
    "plt.plot(range(7,14), all_distances_2, color='b', label='distance of data point {} from cluster 2 documents'.format(data_point))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Building other classification models\n",
    "Because of a uniform syntax in sklearn it is very easy to build other classifiers such as Agglomerative Clustering, Logistic Regression and Support Vector Classifier, K-Nearest Neighbors on the same data. Let's see how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "Hclustering = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\n",
    "Hclustering.fit(train_tfidf.toarray()) #This creates two clusters\n",
    "ms = np.column_stack((train_target,Hclustering.labels_)) #column_stack will simply join two columns. Use print to see what it looks like\n",
    "df = pd.DataFrame(ms, columns = ['Ground truth','Clusters']) #Conversion of array into a dataframe\n",
    "pd.crosstab(df['Ground truth'], df['Clusters'], margins=True) #Creating a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg_classifier = LogisticRegression()\n",
    "logreg_classifier.fit(train_tfidf, train_target)\n",
    "\n",
    "train_predictions = logreg_classifier.predict(train_tfidf)\n",
    "test_predictions = logreg_classifier.predict(test_tfidf)\n",
    "\n",
    "print('Train Accuracy: {}%'.format(accuracy_score(train_predictions, train_target)*100))\n",
    "print('Test Accuracy: {}%'.format(accuracy_score(test_predictions, test_target)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(train_tfidf, train_target)\n",
    "\n",
    "train_predictions = svm_classifier.predict(train_tfidf)\n",
    "test_predictions = svm_classifier.predict(test_tfidf)\n",
    "\n",
    "print('Train Accuracy: {}%'.format(accuracy_score(train_predictions, train_target)*100))\n",
    "print('Test Accuracy: {}%'.format(accuracy_score(test_predictions, test_target)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_tfidf, train_target)\n",
    "\n",
    "test_predictions = knn.predict(test_tfidf)\n",
    "print('Test Accuracy: {}%'.format(accuracy_score(test_predictions, test_target)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Loaded are the text files `train_finance.txt` , `train_medicine.txt` and `train_sports.txt` as `train data and`, `test_finance.txt` , `test_medicine.txt` and `test_sports.txt` as test data from activity_data folder  \n",
    "* Build a K-Means clustering model on train data and see if the test data clusters are as expected.\n",
    "* Note that we don't need the train_categories information since it is an unsupervised algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "os.chdir('activity_data/')\n",
    "train_files = glob.glob('train_*.txt')\n",
    "\n",
    "train_data = []\n",
    "train_categories = []\n",
    "\n",
    "for train_file in train_files:\n",
    "    with open(train_file, 'r') as f:\n",
    "        _data_ = f.readlines()\n",
    "        train_data.extend(_data_)\n",
    "        train_categories.extend([train_file.split('_')[-1].split('.')[0]]*len(_data_))\n",
    "        \n",
    "test_files = glob.glob('test_*.txt')\n",
    "test_data = []\n",
    "test_categories = []\n",
    "for test_file in test_files:\n",
    "    with open(test_file, 'r') as f:\n",
    "        _data_ = f.readlines()\n",
    "        test_data.extend(_data_)\n",
    "        test_categories.extend([test_file.split('_')[-1].split('.')[0]]*len(_data_))\n",
    "        \n",
    "print(len(train_data), len(test_data))\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
