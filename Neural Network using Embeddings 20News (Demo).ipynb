{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''This script loads pre-trained word embeddings (GloVe embeddings)\n",
    "into a Keras Embedding layer, and uses it to\n",
    "train a text classification model on the 20 Newsgroup dataset\n",
    "(classication of newsgroup messages into 20 different categories).\n",
    "GloVe embedding data can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/)\n",
    "20 Newsgroup data can be found at:\n",
    "http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n",
    "Currently used dataset is a subset of it.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "GLOVE_DIR = BASE_DIR\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Loading Glove Vectors to create our own embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors.'.format(len(embeddings_index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 11314 texts.\n"
     ]
    }
   ],
   "source": [
    "print('Processing text dataset')\n",
    "\n",
    "raw_data = pd.read_csv('20news_raw.csv', encoding='latin-1')\n",
    "labels_index = {}\n",
    "for idx, i in enumerate(raw_data.categories.unique()):\n",
    "    labels_index[i] = idx\n",
    "\n",
    "print('Found %s texts.' % len(raw_data.texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data into required format for a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153445 unique tokens.\n",
      "Shape of data tensor: (11314, 1000)\n",
      "Shape of label tensor: (11314, 20)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(raw_data.texts)\n",
    "sequences = tokenizer.texts_to_sequences(raw_data.texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = raw_data.categories\n",
    "labels = [labels_index[label] for label in labels]\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a test train split on a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Embedding Matrix as input for Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model architecture and training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "Train on 9052 samples, validate on 2262 samples\n",
      "Epoch 1/20\n",
      "9052/9052 [==============================] - 5s - loss: 2.6618 - acc: 0.1891 - val_loss: 2.3752 - val_acc: 0.2480\n",
      "Epoch 2/20\n",
      "9052/9052 [==============================] - 5s - loss: 2.1584 - acc: 0.3290 - val_loss: 2.0492 - val_acc: 0.3187\n",
      "Epoch 3/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.9036 - acc: 0.4021 - val_loss: 1.8228 - val_acc: 0.4151\n",
      "Epoch 4/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.7485 - acc: 0.4401 - val_loss: 1.7175 - val_acc: 0.4372\n",
      "Epoch 5/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.6339 - acc: 0.4771 - val_loss: 1.5813 - val_acc: 0.5102\n",
      "Epoch 6/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.5492 - acc: 0.5046 - val_loss: 1.5357 - val_acc: 0.5062\n",
      "Epoch 7/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.4770 - acc: 0.5318 - val_loss: 1.5449 - val_acc: 0.5035\n",
      "Epoch 8/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.4209 - acc: 0.5430 - val_loss: 1.4034 - val_acc: 0.5522\n",
      "Epoch 9/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.3742 - acc: 0.5508 - val_loss: 1.3835 - val_acc: 0.5522\n",
      "Epoch 10/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.3322 - acc: 0.5704 - val_loss: 1.3355 - val_acc: 0.5716\n",
      "Epoch 11/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.3016 - acc: 0.5830 - val_loss: 1.3907 - val_acc: 0.5469\n",
      "Epoch 12/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.2730 - acc: 0.5918 - val_loss: 1.3666 - val_acc: 0.5438\n",
      "Epoch 13/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.2446 - acc: 0.6003 - val_loss: 1.2793 - val_acc: 0.5800\n",
      "Epoch 14/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.2216 - acc: 0.6041 - val_loss: 1.3122 - val_acc: 0.5787\n",
      "Epoch 15/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.1991 - acc: 0.6084 - val_loss: 1.2582 - val_acc: 0.5809\n",
      "Epoch 16/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.1802 - acc: 0.6137 - val_loss: 1.3105 - val_acc: 0.5778\n",
      "Epoch 17/20\n",
      "9052/9052 [==============================] - 5s - loss: 1.1642 - acc: 0.6236 - val_loss: 1.2593 - val_acc: 0.5928\n",
      "Epoch 18/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.1469 - acc: 0.6279 - val_loss: 1.2463 - val_acc: 0.5942\n",
      "Epoch 19/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.1300 - acc: 0.6326 - val_loss: 1.3046 - val_acc: 0.5831\n",
      "Epoch 20/20\n",
      "9052/9052 [==============================] - 4s - loss: 1.1173 - acc: 0.6418 - val_loss: 1.1877 - val_acc: 0.6092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c74fa1f978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    weights=[embedding_matrix],\n",
    "#                     trainable=False\n",
    "                    ))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(50, activation = 'relu'))\n",
    "model.add(Dense(len(labels_index), activation='softmax'))\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "rmsprop = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240/2262 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.187747670316148, 0.60919540235155145]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number in above list is the loss of batch and the second number is the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
