{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading Basic Data\n",
    "Loading a text into python's environment depends largely on the input itself. Following are common ways of storing corpora on disc\n",
    "* Collection of text files in a folder/multiple folders\n",
    "* A single file with each document in a new line\n",
    "* Column(s) of an RDBMS\n",
    "\n",
    "Let's see a couple of examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a text with one document per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/tweets.txt', 'r') as f:\n",
    "    tweets = f.read()\n",
    "    tweets = tweets.split('\\n')\n",
    "    #above two lines could be replace with `tweets = f.readlines()`\n",
    "    \n",
    "print(tweets[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a collection of texts along with their class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir('data/sample_data/') #change directory to where the folders are\n",
    "folders = glob.glob('*') #load all the folder names into a list\n",
    "# print(folders)\n",
    "\n",
    "all_texts = []\n",
    "all_categories = []\n",
    "\n",
    "for folder in folders:\n",
    "    print('importing text files from \"{}\" folder...'.format(folder), end=' ')\n",
    "    \n",
    "    files_in_folder = glob.glob(folder+'/*.txt')\n",
    "    \n",
    "    for _file_ in files_in_folder:\n",
    "        with open(_file_, 'r') as f:\n",
    "            text_in_file = f.read()\n",
    "            all_texts.append(text_in_file)\n",
    "            all_categories.append(folder)\n",
    "            \n",
    "    print('found {} files'.format(len(files_in_folder)))\n",
    "        \n",
    "os.chdir('../..') #revert back to original working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preprocessing\n",
    "Let's load the nltk module and do the following on each text in the `all_texts` list.\n",
    "\n",
    "* Convert everything to lower\n",
    "* Tokenize into sentences\n",
    "* Word tokenize each sentence\n",
    "* Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "stopwords = nltk.corpus.stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "def basic_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word for word in words if word not in eng_stopwords]\n",
    "        tokenized_sentences.append(words)\n",
    "    return(tokenized_sentences)\n",
    "\n",
    "## Testing\n",
    "basic_preprocessing('''This is a test sentence. Second sentence is longer than the previous sentence. \n",
    "NOTICE HOW THE CAPTIALS ARE CONVERTED TO SMALL CASE and how the common words will be removed''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension (Python detour)\n",
    "Notice the statement \n",
    "```\n",
    "words = [word for word in words if word not in eng_stopwords]\n",
    "```\n",
    "We used a combination of `for` and `if` in a single list. This is called a list comprehension. The same code could have been written as follows\n",
    "```python\n",
    "tmp = []\n",
    "for word in words:\n",
    "    if word not in eng_stopwords:\n",
    "       tmp.append(word)\n",
    "tmp = words\n",
    "```\n",
    "The way to read a list comprehension would be \n",
    "```python\n",
    "[f(x) for x in list]\n",
    "```  \n",
    "This means, for every item `x` in the `list`, apply the function `f(x)` and return another list. In our case the function was to return the word itself.\n",
    "\n",
    "`if` is optional. With an if condition the statement would look \n",
    "```python\n",
    "[f(x) for x in list if g(x) else h(x)]\n",
    "```\n",
    "\n",
    "In our case g(x) was \n",
    "\n",
    "```python\n",
    "word not in eng_stopwords\n",
    "```\n",
    "and we skipped h(x) as we don't want the stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_texts = [basic_preprocessing(text) for text in all_texts]\n",
    "print(processed_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "string='''\n",
    "At Waterloo we were fortunate in catching a train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''\n",
    "\n",
    "freqs = FreqDist(word_tokenize(string))\n",
    "print(freqs)\n",
    "\n",
    "print(freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer() #blank tfidf calculator\n",
    "tfidf.fit(tweets) #load the data and store the tf and idf values\n",
    "train_tfidf = tfidf.transform(tweets) #create the tfidf matrix\n",
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the first tweet with everything else and see which one it is closest to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for row_num, row in enumerate(train_tfidf):\n",
    "    print('Similarity between tweet #{} and #{} is: {:.3f}'.format(1, row_num+1, cosine_similarity(train_tfidf[0], row)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like tweet-5 is talking about something similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets[0], tweets[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our own text and compare. This is similar to a query we give and retreive documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tfidf = tfidf.transform(['banks business process blah blah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row_num, row in enumerate(train_tfidf):\n",
    "    print('Similarity between test tweet and tweet #{} is: {:.3f}'.format(row_num, cosine_similarity(test_tfidf[0], row)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity\n",
    "\n",
    "Now that you understand loading, preprocessing and regular expressions, load the data `cognizant_tweets.txt` and do the following \n",
    "1.\tFind the 20 most common words in the corpus  \n",
    "    a.\tExcluding the stop-words  \n",
    "    b.\tIncluding the stop-words  \n",
    "2.\tFind the 10 most common hash-tags in the corpus\n",
    "3.\tReturn a corpus (list of strings) which contains no hashtags or the urls. Hint: Use `word.startswith('string')` method to see if a word has to be removed.  \n",
    "    a.\t(Regex activity - optional) There are a lot of tweets where the letter '&' is written as '&amp;'. Replace the latter with former using re.sub function during pre-processing.\n",
    "4.\tWhat percent of the tweets start with 'RT'?\n",
    "5.  Which tweet is the 5th tweet most similar to? Print it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
