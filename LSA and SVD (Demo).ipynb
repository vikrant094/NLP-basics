{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading modules, data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "# Import all of the scikit learn stuff \n",
    "from __future__ import print_function \n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import Normalizer \n",
    "from sklearn import metrics \n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir('sample_data/') #change directory to where the folders are\n",
    "folders = glob.glob('*') #load all the folder names into a list\n",
    "# print(folders)\n",
    "\n",
    "all_texts = []\n",
    "all_categories = []\n",
    "\n",
    "for folder in folders:\n",
    "    print('importing text files from \"{}\" folder...'.format(folder), end=' ')\n",
    "    \n",
    "    files_in_folder = glob.glob(folder+'/*.txt')\n",
    "    \n",
    "    for _file_ in files_in_folder:\n",
    "        with open(_file_, 'r', encoding='latin-1') as f:\n",
    "            text_in_file = f.read()\n",
    "            all_texts.append(text_in_file)\n",
    "            all_categories.append(folder)\n",
    "            \n",
    "    print('found {} files'.format(len(files_in_folder)))\n",
    "        \n",
    "os.chdir('../') #revert back to original working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "stopwords = nltk.corpus.stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def basic_preprocessing(text):\n",
    "    text = text.lower() #lowering\n",
    "    text = re.sub(r'\\[.*?\\]', '', text) #removing all instances of citation brackets found in wiki articles\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in eng_stopwords] #removing stop words\n",
    "    text = [word for word in text if len(word) > 1] #removing single character tokens\n",
    "#     text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return(text)\n",
    "processed_texts = [basic_preprocessing(text) for text in all_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the TFIDF Matrix of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = TfidfVectorizer()\n",
    "x.fit(all_texts)\n",
    "all_texts_summary = [text[:20] for text in all_texts]\n",
    "tfidf = x.transform(all_texts)\n",
    "pd.DataFrame(tfidf.todense(), index=all_texts_summary, columns=x.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 19\n",
    "lsa = TruncatedSVD(n_topics, algorithm = 'arpack')\n",
    "lsa.fit(tfidf)\n",
    "lsa_data = lsa.transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing concept-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts = ['concept{}'.format(i) for i in range(n_topics)]\n",
    "pd.DataFrame(lsa.components_, columns=x.get_feature_names(), index=concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing document-concept matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lsa_data, index = all_texts_summary, columns=concepts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining document-document similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix_similarity = np.asarray(np.asmatrix(lsa_data) * np.asmatrix(lsa_data).T) \n",
    "pd.DataFrame(matrix_similarity,index=all_texts_summary, columns=all_texts_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LSA using gensim module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "## Creating all {index:word} relations\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "## Converting corpus to a list of indices\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "## Initializing TFIDF parameters from corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "\n",
    "## Creating TFIDF Matrix from data\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "## Creating LSA model on the tfidf\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word = dictionary, num_topics = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(corpus_tfidf.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting each document to it's concept space and using the new vectors for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_corpus = []\n",
    "for lsi_doc in lsi[corpus]:\n",
    "    lsi_corpus.append([topic_component[1] for topic_component in lsi_doc])\n",
    "import numpy as np\n",
    "lsi_corpus = np.array(lsi_corpus)\n",
    "print(lsi_corpus.shape)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "nb_model = BernoulliNB()\n",
    "nb_model.fit(lsi_corpus, all_categories)\n",
    "nb_model.predict(lsi_corpus)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
